{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\qrnts\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Dependencies\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "from DataSource import DataSource\n",
    "from cnn_lstm import CNNLSTMModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options\n",
    "trend_length = 5\n",
    "cnn_length = 200\n",
    "lstm_length = 60\n",
    "symbol = \"ETHUSDT\"\n",
    "start_time = '2024-03-16'\n",
    "end_time = '2024-05-17'\n",
    "interval = '1h'\n",
    "batch_size = 32\n",
    "ohclv_length = 5\n",
    "num_lstm_layers = 8\n",
    "cnn_embedding_length = 128\n",
    "lstm_hidden_dim = 64\n",
    "\n",
    "dataLoader = {\n",
    "    \"trend_length\": trend_length,\n",
    "    \"cnn_length\": cnn_length,\n",
    "    \"lstm_length\": lstm_length,\n",
    "    \"symbol\": symbol,\n",
    "    \"start_time\": start_time,\n",
    "    \"end_time\": end_time,\n",
    "    \"interval\": interval,\n",
    "    \"batch_size\": batch_size\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Data\n",
    "data = DataSource(**dataLoader)\n",
    "\n",
    "with open(\"./trained_Data\", 'wb') as f:\n",
    "    pickle.dump(data, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "with open('./trained_Data', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Model\n",
    "\n",
    "model = CNNLSTMModel(\n",
    "    cnn_length = cnn_length, \n",
    "    cnn_embedding_length = cnn_embedding_length, \n",
    "    ohclv_length = ohclv_length, \n",
    "    lstm_hidden_dim = lstm_hidden_dim, \n",
    "    output_dim= 1,\n",
    "    num_lstm_layers= num_lstm_layers\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.1)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 0.90963\n",
      "[2] loss: 0.90958\n",
      "[3] loss: 0.90954\n",
      "[4] loss: 0.90950\n",
      "[5] loss: 0.90945\n",
      "[6] loss: 0.90941\n",
      "[7] loss: 0.90937\n",
      "[8] loss: 0.90934\n",
      "[9] loss: 0.90930\n",
      "[10] loss: 0.90926\n",
      "[11] loss: 0.90923\n",
      "[12] loss: 0.90919\n",
      "[13] loss: 0.90916\n",
      "[14] loss: 0.90913\n",
      "[15] loss: 0.90909\n",
      "[16] loss: 0.90906\n",
      "[17] loss: 0.90903\n",
      "[18] loss: 0.90900\n",
      "[19] loss: 0.90898\n",
      "[20] loss: 0.90895\n",
      "[21] loss: 0.90892\n",
      "[22] loss: 0.90890\n",
      "[23] loss: 0.90887\n",
      "[24] loss: 0.90884\n",
      "[25] loss: 0.90882\n",
      "[26] loss: 0.90880\n",
      "[27] loss: 0.90877\n",
      "[28] loss: 0.90875\n",
      "[29] loss: 0.90873\n",
      "[30] loss: 0.90871\n",
      "[31] loss: 0.90869\n",
      "[32] loss: 0.90867\n",
      "[33] loss: 0.90865\n",
      "[34] loss: 0.90863\n",
      "[35] loss: 0.90861\n",
      "[36] loss: 0.90860\n",
      "[37] loss: 0.90858\n",
      "[38] loss: 0.90856\n",
      "[39] loss: 0.90855\n",
      "[40] loss: 0.90853\n",
      "[41] loss: 0.90852\n",
      "[42] loss: 0.90850\n",
      "[43] loss: 0.90849\n",
      "[44] loss: 0.90847\n",
      "[45] loss: 0.90846\n",
      "[46] loss: 0.90845\n",
      "[47] loss: 0.90843\n",
      "[48] loss: 0.90842\n",
      "[49] loss: 0.90841\n",
      "[50] loss: 0.90840\n",
      "[51] loss: 0.90838\n",
      "[52] loss: 0.90837\n",
      "[53] loss: 0.90836\n",
      "[54] loss: 0.90835\n",
      "[55] loss: 0.90834\n",
      "[56] loss: 0.90833\n",
      "[57] loss: 0.90832\n",
      "[58] loss: 0.90831\n",
      "[59] loss: 0.90830\n",
      "[60] loss: 0.90829\n",
      "[61] loss: 0.90828\n",
      "[62] loss: 0.90828\n",
      "[63] loss: 0.90827\n",
      "[64] loss: 0.90826\n",
      "[65] loss: 0.90825\n",
      "[66] loss: 0.90824\n",
      "[67] loss: 0.90824\n",
      "[68] loss: 0.90823\n",
      "[69] loss: 0.90822\n",
      "[70] loss: 0.90822\n",
      "[71] loss: 0.90821\n",
      "[72] loss: 0.90820\n",
      "[73] loss: 0.90820\n",
      "[74] loss: 0.90819\n",
      "[75] loss: 0.90818\n",
      "[76] loss: 0.90818\n",
      "[77] loss: 0.90817\n",
      "[78] loss: 0.90817\n",
      "[79] loss: 0.90816\n",
      "[80] loss: 0.90816\n",
      "[81] loss: 0.90815\n",
      "[82] loss: 0.90815\n",
      "[83] loss: 0.90814\n",
      "[84] loss: 0.90814\n",
      "[85] loss: 0.90813\n",
      "[86] loss: 0.90813\n",
      "[87] loss: 0.90812\n",
      "[88] loss: 0.90812\n",
      "[89] loss: 0.90812\n",
      "[90] loss: 0.90811\n",
      "[91] loss: 0.90811\n",
      "[92] loss: 0.90811\n",
      "[93] loss: 0.90810\n",
      "[94] loss: 0.90810\n",
      "[95] loss: 0.90809\n",
      "[96] loss: 0.90809\n",
      "[97] loss: 0.90809\n",
      "[98] loss: 0.90808\n",
      "[99] loss: 0.90808\n",
      "[100] loss: 0.90808\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "epochs = 100\n",
    "inputLoader = data.cnn_data\n",
    "labelLoader = data.label_data\n",
    "n_batch = len(inputLoader)\n",
    "\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times. You can increase if you want.\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    for batch in range(n_batch):\n",
    "        if(batch_size != inputLoader[batch].shape[0]): continue\n",
    "        if(batch_size != labelLoader[batch].shape[0]): continue\n",
    "        inputs = torch.tensor(inputLoader[batch], dtype=torch.float32).to(device)\n",
    "        labels = torch.tensor(labelLoader[batch], dtype=torch.float32).to(device)\n",
    "        ohclv = torch.tensor(data.lstm_data[batch], dtype=torch.float32).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs, ohclv)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    print('[%d] loss: %.5f' %(epoch + 1, epoch_loss/batch_size))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
